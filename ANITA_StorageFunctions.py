# Databricks notebook source
import uuid

# COMMAND ----------

# MAGIC %run "/Shared/Common/Functions/KeyVaultFunctions"

# COMMAND ----------

# DBTITLE 1,getRawFile Function
def getRawFile(container, folder=None, file=None):
  # ##########################################################################################################################  
  # Function: getRawFile
  # Returns a fully qualified name of a file in Raw Zone of Azure Blob Storage
  # 
  # Parameters:
  # container = Container of Azure Blob Storage
  # folder = folder name
  # file = file name including extension
  # 
  # Returns:
  # The fully qualified name of a file in Raw Zone of Azure Blob Storage
  # ##########################################################################################################################  
  rawZone = getSecret("RawZoneStorageAccountName")
  rawZoneKey = getSecret("RawZoneStorageAccessKey")
  spark.conf.set("fs.azure.account.key." + rawZone + ".blob.core.windows.net", rawZoneKey)
  
  if folder is not None and file is not None:
    rawFile ="wasbs://" + container + "@"+ rawZone + ".blob.core.windows.net/" + folder + "/" + file
  elif folder is None and file is not None:
    rawFile ="wasbs://" + container + "@"+ rawZone + ".blob.core.windows.net/" + file
  elif folder is not None and file is None:
    rawFile ="wasbs://" + container + "@"+ rawZone + ".blob.core.windows.net/" + folder
  return rawFile

# COMMAND ----------

# DBTITLE 1,getC1File Function
def getC1File(container, folder=None, file=None):
  # ##########################################################################################################################  
  # Function: getC1File
  # Returns a fully qualified name of a file in Curated1 Zone of Azure Blob Storage
  # 
  # Parameters:
  # container = Container of Azure Blob Storage
  # folder = folder name
  # file = file name including extension
  # 
  # Returns:
  # The fully qualified name of a file in Curated1 Zone of Azure Blob Storage
  # ##########################################################################################################################    
  curated1Zone = getSecret("CuratedZoneStorageAccountName")
  curated1ZoneKey = getSecret("CuratedZoneStorageAccessKey")
  spark.conf.set("fs.azure.account.key." + curated1Zone + ".blob.core.windows.net", curated1ZoneKey)
  
  if folder is not None and file is not None:
    c1File ="wasbs://" + container + "@"+ curated1Zone + ".blob.core.windows.net/" + folder + "/" + file
  elif folder is None and file is not None:
    c1File ="wasbs://" + container + "@"+ curated1Zone + ".blob.core.windows.net/" + file
  elif folder is not None and file is None:
    c1File ="wasbs://" + container + "@"+ curated1Zone + ".blob.core.windows.net/" + folder
  return c1File

# COMMAND ----------

# DBTITLE 1,getC2File Function
def getC2File(container, folder=None, file=None):
  # ##########################################################################################################################  
  # Function: getC2File
  # Returns a fully qualified name of a file in Curated2 Zone of Azure Blob Storage
  # 
  # Parameters:
  # container = Container of Azure Blob Storage
  # folder = folder name
  # file = file name including extension
  # 
  # Returns:
  # The fully qualified name of a file in Curated2 Zone of Azure Blob Storage
  # ##########################################################################################################################    
  curated2Zone = getSecret("Curated2ZoneStorageAccountName")
  curated2ZoneKey = getSecret("Curated2ZoneStorageAccessKey")
  spark.conf.set("fs.azure.account.key." + curated2Zone + ".blob.core.windows.net", curated2ZoneKey)
  
  if folder is not None and file is not None:
    c2File ="wasbs://" + container + "@"+ curated2Zone + ".blob.core.windows.net/" + folder + "/" + file
  elif folder is None and file is not None:
    c2File ="wasbs://" + container + "@"+ curated2Zone + ".blob.core.windows.net/" + file
  elif folder is not None and file is None:
    c2File ="wasbs://" + container + "@"+ curated2Zone + ".blob.core.windows.net/" + folder
  return c2File

# COMMAND ----------

# DBTITLE 1,getTempSQLDWFolder Function
def getTempSQLDWFolder():
  # ######################################################################################################################################      
  # Function: getTempSQLDWFolder
  #  
  # Parameters:
  # None
  # 
  # Returns:
  # The fully qualified name of a Drop Zone folder in Blob Storage where temp parquet files generated by SQL DW connector is stored
  # ######################################################################################################################################        
  systemStorage = getSecret("SystemStorageAccountName")
  systemStorageKey = getSecret("SystemStorageAccessKey")
  guid = str(uuid.uuid4())
  spark.conf.set("fs.azure.account.key." + systemStorage + ".blob.core.windows.net", systemStorageKey)
  tempSQLDWFolder= "wasbs://tempdir@"+ systemStorage + ".blob.core.windows.net/" + guid
  return tempSQLDWFolder

# COMMAND ----------

# DBTITLE 1,readRawFile Function
def readRawFile(container, folder=None, file=None, colSeparator=None, headerFlag=None):
  # ##########################################################################################################################  
  # Function: readRawFile
  # Reads a file from raw zone of Azure Blob Storage and returns as dataframe
  # 
  # Parameters:
  # container = File System/Container of Azure Blob Storage
  # folder = folder name
  # file = file name including extension
  # colSeparator = Column separator for text files
  # headerFlag = boolean flag to indicate whether the text file has a header or not  
  # 
  # Returns:
  # A dataframe of the raw file
  # ##########################################################################################################################    
  rawFile = getRawFile(container, folder, file)
  if ".csv" in rawFile or ".txt" in rawFile:
    df = spark.read.csv(path=rawFile, sep=colSeparator, header=headerFlag, inferSchema="true", multiLine="true")
  elif ".parquet" in rawFile:
    df = spark.read.parquet(rawFile)
  elif ".orc" in rawFile:
    df = spark.read.orc(rawFile)
  elif ".json" in rawFile:
    df = spark.read.json(rawFile)
  else:
    df = spark.read.format("csv").load(rawFile)
  
  df =df.dropDuplicates()
  return df

# COMMAND ----------

# DBTITLE 1,readC1File Function
def readC1File(container, folder=None, file=None, colSeparator=None, headerFlag=None):
  # ##########################################################################################################################  
  # Function: readC1File
  # Reads a file from curated1 zone of Azure Blob Storage and returns as dataframe
  # 
  # Parameters:
  # container = Container of Azure Blob Storage
  # folder = folder name
  # file = file name including extension
  # colSeparator = Column separator for text files
  # headerFlag = boolean flag to indicate whether the text file has a header or not  
  # 
  # Returns:
  # A dataframe of the curated1  file
  # ##########################################################################################################################    
  c1File = getC1File(container, folder, file)
  if ".csv" in c1File or ".txt" in c1File:
    df = spark.read.csv(path=c1File, sep=colSeparator, header=headerFlag, inferSchema="true")
  elif ".parquet" in c1File:
    df = spark.read.parquet(c1File)
  elif ".orc" in c1File:
    df = spark.read.orc(c1File)
  elif "json" in c1File:
    df = spark.read.json(path=c1File,multiLine="true")
  else:
    df = spark.read.format("csv").load(c1File)
  
  df =df.dropDuplicates()
  return df

# COMMAND ----------

# DBTITLE 1,readC2File Function
def readC2File(container, folder=None, file=None, colSeparator=None, headerFlag=None):
  # ##########################################################################################################################  
  # Function: readC1File
  # Reads a file from curated2 zone of Azure Blob Storage and returns as dataframe
  # 
  # Parameters:
  # container = Container of Azure Blob Storage
  # folder = folder name
  # file = file name including extension
  # colSeparator = Column separator for text files
  # headerFlag = boolean flag to indicate whether the text file has a header or not  
  # 
  # Returns:
  # A dataframe of the curated2 file
  # ##########################################################################################################################    
  c2File = getC2File(container, folder, file)
  if ".csv" in c2File or ".txt" in c2File:
    df = spark.read.csv(path=c2File, sep=colSeparator, header=headerFlag, inferSchema="true")
  elif ".parquet" in c2File:
    df = spark.read.parquet(c2File)
  elif ".orc" in c2File:
    df = spark.read.orc(c2File)
  elif "json" in c1File:
    df = spark.read.json(path=c1File,multiLine="true")    
  else:
    df = spark.read.format("csv").load(c2File)
  
  df =df.dropDuplicates()
  return df

# COMMAND ----------

# DBTITLE 1,writeC1File Function
def writeC1File(df,container, folder=None, file=None, fileFormat=None, writeMode=None,colSeparator=None):
  # ##########################################################################################################################  
  # Function: writeC1File
  # Writes the input dataframe to a file in curated1 zone of Azure Blob
  # 
  # Parameters:
  # df= input dataframe
  # container = File System/Container of Azure Blob Storage
  # folder = folder name
  # file = file name including extension
  # fileFormat = File extension. Supported formats are csv/txt/parquet/orc/json  
  # writeMode= mode of writing the curated file. Allowed values - append/overwrite/ignore/error/errorifexists
  # colSeparator = Column separator for text files
  # 
  # Returns:
  # A dataframe of the raw file
  # ########################################################################################################################## 
  c1File= getC1File(container, folder, file)
  if "csv" in fileFormat or 'txt' in fileFormat:
    df.write.csv(c1File,mode=writeMode,sep=colSeparator,header="true", nullValue="0", timestampFormat ="yyyy-MM-dd HH:mm:ss")
  elif "parquet" in fileFormat:
    df.write.parquet(c1File,mode=writeMode)
  elif "orc" in fileFormat:
    df.write.orc(c1File,mode=writeMode)
  elif "json" in fileFormat:
    df.write.json(c1File, mode=writeMode)
  else:
    df.write.save(path=c1File,format=fileFormat,mode=writeMode)
  return

# COMMAND ----------

# DBTITLE 1,writeC2File Functions
def writeC2File(df,container, folder=None, file=None, fileFormat=None, writeMode=None,colSeparator=None):
  # ##########################################################################################################################  
  # Function: writeC2File
  # Writes the input dataframe to a file in curated2 zone of Azure Blob Storage
  # 
  # Parameters:
  # df= input dataframe
  # container = File System/Container of Azure Blob Storage
  # folder = folder name
  # file = file name including extension
  # fileFormat = File extension. Supported formats are csv/txt/parquet/orc/json  
  # writeMode= mode of writing the curated file. Allowed values - append/overwrite/ignore/error/errorifexists
  # colSeparator = Column separator for text files
  # 
  # Returns:
  # A dataframe of the raw file
  # ########################################################################################################################## 
  c2File= getC2File(container, folder, file)
  if "csv" in fileFormat or 'txt' in fileFormat:
    df.write.csv(c2File,mode=writeMode,sep=colSeparator,header="true", nullValue="0", timestampFormat ="yyyy-MM-dd HH:mm:ss")
  elif "parquet" in fileFormat:
    df.write.parquet(c2File,mode=writeMode)
  elif "orc" in fileFormat:
    df.write.orc(c2File,mode=writeMode)
  elif "json" in fileFormat:
    df.write.json(c2File, mode=writeMode)    
  else:
    df.write.save(path=c2File,format=fileFormat,mode=writeMode)
  return

# COMMAND ----------

# DBTITLE 1,createCheckpointDir
def createCheckpointDir():
  # #####################################################################################################################################################################################
  # Function: createCheckpointDir
  #  Creates a checkpoint folder for spark in drop-zone
  #  Call this function to 
  #     1. Materialize dataframe to a permanent storage as an additional means of redundancy for long running jobs and recover from point of failure
  #     2. Truncate execution plan for recursive calculations on spark dataframes which might put memory pressure leading to Out of Memory (OOM) exception 
  # 
  # Parameters:
  #  None
  # 
  # Returns:
  #  The folder name where the checkpoint files will be created
  # ##################################################################################################################################################################################### 
  tempDirMountPoint="/mnt/tempdir"
  systemStorage = getSecret("SystemStorageAccountName")
  systemStorageKey = getSecret("SystemStorageAccessKey")
  guid = str(uuid.uuid4())

  #Create mount point to drop-zone if it doesn't exist
  if not any(mount.mountPoint==tempDirMountPoint for mount in dbutils.fs.mounts()):
    dbutils.fs.mount( source = "wasbs://tempdir@"+ dropZone + ".blob.core.windows.net",
                    mount_point = tempDirMountPoint,
                    extra_configs = {"fs.azure.account.key."+ systemStorage + ".blob.core.windows.net":systemStorageKey})

  
  #display(dbutils.fs.mounts())
  checkpointDir= "dbfs:"+ tempDirMountPoint + "/sparkCheckpointDir/" + guid
  #Create folder if it doesn't exist and set spark checkpoint directory
  dbutils.fs.mkdirs(checkpointDir)
  spark.sparkContext.setCheckpointDir(checkpointDir)
  return checkpointDir
